\section{Experimental Results}

In this section, we present a further analysis on our sampling algorithm. Also, by comparing the test accuracy between models trained on subgraphs sampled by our algorithm and random baselines, we conclude the effectiveness and robustness of our proposed method. 

% To execute this algorithm, we need a graph and a sample rate $\gamma$ as input. We firstly denote $N$ as the number of nodes to exclude, and then calculate feature score $S$ as the diagonal of $XX^TXX^T$. After that, we obtain the sorted index of $S$ and our subgraph $\tilde{G}$ accordingly, over which we will be training with better efficiency. 

\noindent \textbf{Trace preservation.} Among many ways to evaluate a sampling algorithm over a graph, apart from test accuracy benchmarks which we will show in Figure \ref{fig:GNN Acc}, one of the most frequently used, and probably most informative one is the rank of $\mathbf{L}$. However, because the estimation on the rank of $\mathbf{L}$ is almost infeasible due to the high dimensionality, trace is often used as a mediate. With the lower bound we proposed in Proposition \ref{bound on tr L}, empirically from Figure \ref{fig:trace} we see that across all three datasets, our sampling method results in subgraphs with larger adjusted $\tilde{\text{tr}}(\textbf{L}) = \text{tr}(\mathbf{L}) / n$  at almost all sample rates compared with the average of random baselines. This pattern is especially obvious over PubMed dataset.

\noindent \textbf{GNN training.} As for GNN experiments, for each dataset at a given sample rate, we firstly tune the best configuration for our sampling algorithm as shown in the following, then test the same configuration on random baselines with 50 different runs. Specifically, the valid/test accuracy obtained in the training and evaluation process is based on original full-size graph, which means we only limit the training process to the sampled subgraphs. This pipeline is ideal because it simulates the process of training and model selection, which makes the experiments more indicative for general purposes. 

\noindent \textbf{Experiment details.} For each dataset and sample rate, we chose hidden dimension in $\{64, 128\}$, number of layers in $\{1,2,3\}$, number of epochs in $\{200, 300\}$, learning rate and weight decay in $\{0.001, 0.0001\}$, GCN model type in $\{\text{GCN}, \text{SAGE}\}$ with all ReLU activation between all layers, and used Adam as our optimizer. All of the graphs are configured to be undirected by adding edges of the opposite direction before being fed into the networks. 

It is worth mentioning that the GNN of our choice is rather simple. This is because the power of a sampling method can be more apparent and easier to identify if model complexity is relatively restrained. In Figure \ref{fig:GNN Acc}, where the box plots represent the distribution of random baseline accuracy, the test accuracy acquired by our model(the red dots) are all better by a distinctive margin except for Citeseer at sample rate 25\%. In other cases, for instance, all three datasets at a sample rate of 62.5\%, our heuristic won by around 10\% of accuracy.