\section{Introduction}
\label{sec:intro}

Graph neural networks (GNNs) are deep neural networks tailored to network data which have shown great empirical performance in several graph machine learning tasks \cite{gori2005new,kipf17-classifgcnn,defferrard17-cnngraphs,gama18-gnnarchit}. This is especially true in graph signal processing problems---such as recommender systems on product similarity networks \cite{ruiz2020gnns}, or attribution of research papers to scientific domains \cite{hamilton2017inductive}---in which GNNs' invariance and stability properties play a key role. 

Yet, in practice most successful applications of GNNs are limited to graphs of moderate size. The sheer size of many modern networks, typically in the order of several millions, frequently makes these models impractical to train. Good results have been seen by leveraging the GNN's transferability property \cite{ruiz20-transf,levie2019transferability}, which states that a GNN with fixed weights produces similar outputs on large enough graphs belonging to the same ``family'', e.g., the same random graph model. This property allows training the GNN on a graph of moderate size, and subsequently transferring it for inference on the large graph.

The transferability of GNNs is closely related to their convolutional parametrization, and is a consequence of the fact that graph convolutions converge on sequences of graphs converging to a common graph limit \cite{ruiz2020graphonsp}. Under certain assumptions on the type of limit, and on how the graphs converge to (or are sampled from) them, it is possible to obtain non-asymptotic error bounds inversely proportional to the sizes of the graphs. Such bounds are then used to inform practical considerations, such as the minimum graph size on which to train a GNN to meet a maximum transference error. Once this is determined, the training graph is obtained by sampling a subgraph of the appropriate size at random from the large graph.

Learning GNNs on randomly subsampled graphs works reasonably well on average but, for models trained on small subsamples, there is large variance in performance, and the worst-case performance can be quite low; see Figure \ref{fig:GNN Acc}. While this is a natural consequence of subsampling any type of data, on graphs these issues are exacerbated by the fact that the random node-induced subgraphs usually have disconnected components and isolated nodes. This leads to loss of rank, which in turn affects the expressive power of GNNs \cite{ruiz2024spectral}.
Graph sampling algorithms better at preserving matrix rank---such as spectral algorithms, e.g. \cite{chen2015discrete,anis2016efficient}---or connectivity---such as local algorithms, e.g., breadth-first search---exist, however, spectral methods have high computational complexity, and local methods can be myopic, focusing too much on specific regions of the graph.

In this paper, we identify a property of graphs that allows them to be sampled more efficiently and without restricting to local regions: feature homophily. More specifically, let $G=(V,E)$ be a graph with node features $X \in \reals^{|V| \times d}$. This graph is said to be feature-homophilic if, given that $(i,j)$ is an edge in $G$, the node features $X[i,:]$ and $X[j,:]$ are close. Our first contribution is to introduce a novel mathematical definition of feature homophily based on the graph Laplacian. Then, we show that, by sampling nodes that minimize the trace of the correlation matrix $XX^T$, it is possible to improve the trace of the graph Laplacian, which is directly related to graph rank, on homophilic graphs. 
This heuristic is formalized as a graph sampling algorithm in Algorithm \ref{alg:sampling}. Unlike other graph sampling routines, it does not require sequential node additions/deletions, and has complexity $O(|V||E|)$, which is substantially cheaper than other graph sampling algorithms for large $|V|$ and moderate $d$. 

We conclude with an experimental study of the proposed algorithm on homophilic citation networks, in which we compare it with random sampling. We observe that, for the same sampling budget, our algorithm preserves trace better than sampling at random, and leads to better transferability performance in a semi-supervised learning task.

\begin{comment}
Graph theory plays a pivotal role in the analysis of complex systems, such as social relation networks\cite{majeed2020graph}, genomics and proteomics\cite{pavlopoulos2011using}\cite{wu2014pathway}, and communication network design\cite{jiang2022graph}. While the advent of big data introduces various graph-structured data, large-scale graphs pose challenges in efficient computation. The most frequently referred citation datasets, Cora, Citeseer, and Pubmed\cite{yang2016revisiting} all consist of thousands of nodes, resulting in huge adjacency and Laplacian matrices. Because matrix multiplication involving these two matrices are ubiquitous in graph-related algorithms, an effective downsampling method on nodes will significantly boost the efficiency of existing algorithms. 

Node downsampling is a technique used to shrink the size of the graph while maintaining the overall structure and information unchanged. It attempts to detect the most trivial nodes and exclude them from the graph, alongside with their edges. There are numerous works in node downsampling, ranging from random sampling to more sophisticated techniques like spectral theory\cite{narang2011downsampling} and maximum spanning tree\cite{nguyen2014downsampling}. Although many researchers tried to revail how information is stored and passed through the graph structure, only little attention has been put on the feature space. In this study, we aim to address this limitation by introducing an algorithm that corporates both the graph structure and the node features. 

Our proposed methodology leverages the combination of graph theory principles and trace optimization techniques. In particular, we introduce the concept of feature homophily to represent the alignment between features and labels. Originally, homophily was defined to evaluate the degree of alignment between edges and labels. In other words, graph is considered homophilic if the majority of the edges connect nodes with the same label. For example, social relation network is generally homophilic because people with close connections tend to share similar characteristics like hobbies, education levels, and income\cite{mcpherson2001birds}. On the other hand, dating networks are mostly heterophilic because the majority of the population prone to dating people of the opposite gender\cite{zhu2021graph}. Our work focuses on optimization over feature homophily in the process of downsampling nodes. 

This study makes several significant contributions:
\begin{enumerate}[itemsep=-3pt]
    \item We propose a novel sampling algorithm for training GNNs on large graphs that only requires a one-time computation per dataset.
    \item We generalize the definition of graph homophily to feature homophily which measures the alignment between the graph structure and node features.
    \item We demonstrate a trace optimization heuristic that works well in our context.  
\end{enumerate}

Our theoretical results provide a lower bound on the trace of the Laplacian in terms of feature homophily. The empirical results show that for the same configurations, GNNs trained on subgraphs generated by our sampling method attain test accuracy $5\sim10\%$ higher than the average of 100 random baselines. Some of the gaps even reached around 20\%. 
%  \red{L: Make sure to skim through the commands in the my\_symbol.sty file. For instance, to write bold $x$, use the command ``backslash bbx'': $\bbx$.}
% These guidelines include complete descriptions of the fonts, spacing, and
% related information for producing your proceedings manuscripts. Please follow
% them and if you have any questions, direct them to Conference Management
% Services, Inc.: Phone +1-979-846-6800 or email
% to \\\texttt{papers@2024.ieeeicassp.org}.
\end{comment}